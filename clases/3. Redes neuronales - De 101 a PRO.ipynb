{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Redes neuronales - De 101 a PRO.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "8ygX408AWx_J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Redes neuronales - De 101 a PRO\n",
        "\n",
        "¡Bienvenidos a la tercera sesión! Tras la paliza de ayer con el backpropagation y cómo funcionan realmente las redes neuronales, hoy vamos a ver lo siguiente:\n",
        "\n",
        "**Redes neuronales 2**\n",
        "* Teoría de optimización. Gradient Descent (GD, SGD, Mini-batch GD).\n",
        "* Learning rate y batch size.\n",
        "* Funciones de pérdidas y funciones de activación.\n",
        "* Inicialización de los pesos"
      ]
    },
    {
      "metadata": {
        "id": "mY275LXJXeTK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Teoría de optimización: Gradient Descent, Stochastic Gradient Descent, Mini-batch Gradient Descent\n",
        "\n",
        "Ayer estuvimos viendo cómo funciona el gradient descent, os acordáis, ¿verdad? Cómo olvidarlo...eh? Bueno, tranquilos porque la clase de hoy es mucho mas *light*: vamos a ver diferentes formas de utilizar el gradient descent para ganar velocidad.\n",
        "\n",
        "Necesito que recordéis cómo funciona el descenso del gradiente: calculando el calculando el error de todas las muestras de nuestro training y luego actualizando los pesos en la dirección que indica el gradiente respecto a cada uno de ellos. Es decir, necesitamos, **para cada época, calcular todas las predicciones (etapa forward), luego los errores, y luego propagar los errores hacia atrás para ver cuanto influye cada peso en ese error y actualizarlo en consecuencia.**\n",
        "\n",
        "Imagináos que:\n",
        "* tenemos un dataset de 100.000 muestras\n",
        "* que cada etapa forward tarda 2ms\n",
        "* cada cálculo del error 1ms\n",
        "* cada etapa de backpropagation tarda 3ms\n",
        "\n",
        "Si hacemos el cálculo, tenemos que:\n",
        "\n",
        "**Tiempo por época** $= (2ms+1ms+3ms)·100.000$ muestras $= 600.000ms = 600$ segundos $=$ **10 minutos**\n",
        "\n",
        "De normal, una red puede requerir cientos, si no miles de épocas, para conseguir una convergencia adecuada. Pongamos que necesitamos 100 épocas, que es un número bajo. ¿Cuánto tardaría en total en entrenarse la red?\n",
        "\n",
        "**Tiempo total de entrenamiento** $=10$ minutos $· 100$ épocas $= 1.000$ minutos $=$ **16'6 horas !!!**\n",
        "\n",
        "No parece muy cómodo tener que esperar más de 16 horas para ver los resultados de una red, verdad? Y hemos sido buenos suponiendo que teníamos solo 100.000 muestras. ImageNet, por ejemplo, consta de 1.2 millones de imágenes, lo que nos costaría 2h por época, o lo que es lo mismo, 8.3 días, **más de una semana para ver el comportamiento de una red.** \n",
        "\n",
        "Veis la necesidad, ¿no? ¿Se os ocurre alguna forma de disminuir este tiempo?\n",
        "\n",
        "Una forma de reducir drásticamente el tiempo necesario sería utilizar una sola muestra escogida aleatoriamente cada vez que quisieramos actualizar los pesos. De esta forma, para actualizar los pesos simplemente tendríamos que calcular las predicciones, errores y backpropagation de una muestra. Esto reduciría nuestro tiempo total a:\n",
        "\n",
        "**Tiempo total de entrenamiento** $=(2ms+1ms+3ms)·1$ muestra $·100$ épocas $= 600ms = $ **0,6s**\n",
        "\n",
        "### Eureka! Qué maravilla! Acabamos de arreglar el mundo!\n",
        "\n",
        "A este método se le conoce como **Stochastic Gradient Descent**, y estoy seguro de que todos vosotros os habéis dado cuenta de que tiene una **desventaja MUY IMPORTANTE**. ¿Quién me la dice?\n",
        "\n",
        "Os voy a dar una pista:\n",
        "\n",
        "### ¿Cuál creéis que es el camino seguido por el Gradient Descent, y cual el seguido por el Stochastic Gradient Descent?\n",
        "<img src=\"https://image.ibb.co/hiK2BT/mountain_gd_sgd_mbsgd_hidden.png\" alt=\"mountain_gd_sgd_mbsgd_hidden\" border=\"0\">\n",
        "<!--\n",
        "<img src=\"https://image.ibb.co/n2RFWT/mountain_gd_sgd_mbsgd.png\" alt=\"mountain_gd_sgd_mbsgd\" border=\"0\">\n",
        "<!-- <img src=\"https://image.ibb.co/jZHNBT/contours_gd_sgd_mbsgd.png\" alt=\"contours_gd_sgd_mbsgd\" border=\"0\"> -->\n",
        "\n",
        "El camino rojo es el que sigue el gradient descent, que al calcular el gradiente usando todas las muestras del dataset consigue unas actualizaciones coherentes siempre en la dirección que permite minimizar el error.\n",
        "\n",
        "Y si! El camino magenta es el seguido por el SGD. ¿Qué es lo que está pasando? Que cada actualización de los pesos se hace para minimizar el error teniendo en cuenta solo una muestra, así que lo que minimizamos es el error para esa muestra en particular. Por eso tiene un comportamiento más caótico y le cuesta más converger, aunque, a cambio, se ejecuta mucho más rápido, por lo que en el tiempo que el GD necesita para ejecutar una época, el SGD puede ejecutar miles.\n",
        "\n",
        "**Como siempre, la virtud se encuentra en el término medio. Véis la linea verde, verdad? Esa es la buena! ¿Cómo llegamos a ella?**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8tg1hAR6BRgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bueno, vamos a pensar qué tenemos hasta ahora. Tenemos:\n",
        "\n",
        "* Un método que calcula las predicciones y errores de **todos** los elementos de nuestro training set: **(Vanilla) Gradient Descent**\n",
        "* Un método que calcula las predicciones y errores de **1 elemento** escogido aleatoriamente de nuestro training set: **Stochastic Gradient Descent**\n",
        "\n",
        "¿Qué os parecería que en vez de **1 solo elemento**, escogiésemos **N elementos**? De esta forma:\n",
        "\n",
        "* Aumentamos la **estabilidad** del algoritmo, ya que no solo nos fijamos en un solo elemento, sino en $k$ (es decir, disminuímos los cambios de dirección tan abruptos y caóticos que tiene la línea magenta)\n",
        "* Disminuimos el **tiempo de ejecución** con respecto al gradient descent tradicional, pues pasamos de las $N$ muestras que tiene nuestro training set, a $k$, donde $k << N$\n",
        "\n",
        "Parece una alternativa interesante, no creéis? Este método se conoce como **Mini-batch Stochastic Gradient Descent**, y es realmente el que se utiliza en la práctica.\n",
        "\n",
        "Normalmente, $k$ se elige para que sea una potencia de 2, ya que eso permite aprovechar algunas optimizaciones que tienen las GPUs implementadas para estos casos. Un $k$ típico podría ser $k=256$, pero al final esto lo limita la memoria de la GPU.\n",
        "\n",
        "Cuanto más bajo sea $k$, más se parecerá al SGD puro, y más épocas le costará convergir, aunque también es verdad que las calculará más rápido.\n",
        "\n",
        "Y a la inversa, cuanto más alto sea $k$, más se parecerá al GD puro, y más le costará calcular cada época, pero necesitará menos para convergir.\n",
        "\n",
        "### Pues con esto ya lo sabéis todo sobre el Gradient Descent y los diferentes tipos de implementaciones!\n",
        "\n",
        "Vamos a ver ahora qué es el learning rate y el batch size, y cómo influyen.\n"
      ]
    },
    {
      "metadata": {
        "id": "g4xcQlXrEZIR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Learning rate y batch size\n",
        "\n",
        "El learning rate y el batch size son dos parámetros directamente relacionados con el algoritmo del gradient descent.\n",
        "\n",
        "### 3.2.1 Learning rate\n",
        "\n",
        "Como ya sabéis de la anterior sesión, la forma de actualizar los pesos es mediante estas fórmulas:\n",
        "\n",
        "<center><img src=\"https://image.ibb.co/jMmHRT/net_weights_update.png\" alt=\"net_weights_update\" border=\"0\" height=\"250\"></center>\n",
        "\n",
        "¿Os acordáis, verdad? Lo que multiplica al $\\frac{\\partial E_{total}}{\\partial w_n}$, llamado $\\eta$, es el **learning rate**, que es lo que indica **la importancia** que le damos al error para actualizar cada peso, es decir, la **rapidez** o cómo de abruptos son los cambios en los pesos.\n",
        "\n",
        "Así, un $\\eta$ muy alto, hará que los cambios en los pesos sean muy grandes de una iteración a otra, lo que tiene el problema de que podemos llegar a saltarnos nuestro mínimo.\n",
        "\n",
        "Fijaos, con esta imagen se ve estupendamente:\n",
        "\n",
        "<img src=\"https://image.ibb.co/ncnAY8/learning_rate_eta.png\" alt=\"learning_rate_eta\" border=\"0\" height=\"150\">\n",
        "\n",
        "Otra posibilidad es establecer un $\\eta$ muy bajo, lo que haría que nuestra red necesitara muchísimas épocas para llegar a un mínimo aceptable. Además, correríamos el riesgo de quedarnos atrapados en un mínimo peor del mejor que podríamos conseguir con un $\\eta$ más alto.\n",
        "\n",
        "<img src=\"https://image.ibb.co/frt3Lo/learning_rate_eta_low.gif\" alt=\"learning_rate_eta_low\" border=\"0\" height=\"200\">\n",
        "\n",
        "Vamos a hacer un pequeño inciso para hablar sobre los mínimos: lo que conseguimos con una **red neuronal**, normalmente, no es alcanzar el mínimo global de nuestra función, sino que **alcanzamos un mínimo local lo suficientemente bueno como para realizar correctamente la tarea que estamos desarrollando**.\n",
        "\n",
        "Tras haber aclarado esto, queda patente lo importante que es conseguir un learning rate adecuado, verdad? Y cómo lo hacemos? Primero, qué es lo que buscamos? Fijaos en esta imagen:\n",
        "\n",
        "<img src=\"https://image.ibb.co/heYQY8/learning_rate_eta_graph.png\" alt=\"learning_rate_eta_graph\" border=\"0\" height=\"250\">\n",
        "\n",
        "Lo que queremos es un *learning rate* óptimo, que nos permita ir reduciendo el error conforme van pasando las épocas, hasta llegar a nuestro mínimo buscado. En la gráfica, este *learning rate* sería la línea roja. ¿Y cómo conseguimos que nuestro learning rate sea el óptimo?\n",
        "\n",
        "Pues una opción muy utilizada es aplicar un **decrecimiento** o *decay* a nuestro learning rate **conforme más va disminuyendo la función de pérdidas** (lo que indica que estamos llegando al mínimo buscado).\n",
        "\n",
        "<img src=\"https://image.ibb.co/mdBUt8/learning_rate_eta_decreasing.png\" alt=\"learning_rate_eta_decreasing\" border=\"0\" height=\"250\">\n",
        "\n",
        "De esta forma, evitamos morirnos de viejos esperando a que converja por haber elegido un learning rate muy bajito, y evitamos saltarnos nuestro mínimo porque cuanto más cerca estamos de él más pequeños son los pasos que damos hacia él.\n",
        "\n",
        "Vamos a ver cómo se comportaría la última red que implementamos (la que trabajaba con el MNIST) con **diferentes learning rates**:"
      ]
    },
    {
      "metadata": {
        "id": "9jw55ghdTl5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Los valores típicos de learning rate suelen ser: \n",
        "\n",
        "* 0.1\n",
        "* 0.01\n",
        "* 0.001\n",
        "\n",
        "A partir de ahí, nos toca a nosotros *tunear* el learning rate según lo que veamos que ocurre."
      ]
    },
    {
      "metadata": {
        "id": "Fljl_4hQSw27",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ejemplos learning rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OUZYyV6rV6yh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bueno, pues la mejor ejecución la hemos conseguido con learning rate = 0.1. Pero... ¿se puede mejorar?\n",
        "\n",
        "Como podréis imaginaros, el learning rate es un parámetro muy importante. Ya hemos visto antes que si ponemos un learning rate muy alto, podemos saltarnos el mínimo. En cambio, si ponemos un learning rate muy bajo, igual nos morimos de viejos antes de ver el resultado. ¿Cómo podemos arreglar esto? Pues una forma sería dar pasos más grandes al principio, y conforme nos aproximamos a la meta, empezar a disminuir su longitud para no pasárnosla, no os parece?\n",
        "\n",
        "Pues esto mismo se llama learning rate *decay*. ¿Qué os parece si lo implementamos en nuestro ejemplo?\n",
        "\n",
        "Vamos allá!"
      ]
    },
    {
      "metadata": {
        "id": "Je-NzPCRWdso",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ejemplo con decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zHrdVpJDWcxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fijaos que con esta simple mejora, hemos sido capaces de mejorar la precisión un poco con respecto al learning rate óptimo, que habíamos definido como 0.1. Y existen métodos más complejos de aplicar el *decay* que producen mejores resultados.\n",
        "\n",
        "**Así que ya sabéis, mucha atención al learning rate!!**\n",
        "\n",
        "**IMPORTANTE**: Cabe decir que esta implementación de learning rate *decay* es muy básica, tanto, que se cambia el learning rate para cada batch dentro de una época. Esto no es lo mejor, pero lo hemos hecho así por no complicar la programación. **Lo adecuado sería que se actualizara el learning rate para cada época**, no para cada batch. Tranquiiiilos, lo veremos en la siguiente sesión! ;-)\n"
      ]
    },
    {
      "metadata": {
        "id": "hlXGxShTMytR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Batch size\n",
        "\n",
        "Vamos a ver ahora qué es el **batch size**. ¿Recordáis cuando hemos hablado antes del SGD y el Mini-batch SGD? \n",
        "\n",
        "Recordad que el SGD es un Mini-bacth SGD donde $k=1$.\n",
        "\n",
        "Y que en el Mini-bacth SGD la $k$ indica el número de muestras que se utilizan para actualizar los pesos cada vez. Realmente, este no es un parámetro crítico y se suele establecer como el **número máximo de muestras potencia de 2 que caben en nuestra GPU**.\n",
        "\n",
        "Ejemplos:\n",
        "\n",
        "* Tenemos una GPU con 8GB de memoria, cuantas muestras nos caben si cada imagen ocupa 1MB?\n",
        "\n",
        "Bueno, pues no es tan fácil! Resulta que depende de la arquitectura de la red. Las capas *Densas* o *Fully Connected*, que son las tradicionales en las que todas las neuronas se interconectan con todas las neuronas de la siguiente capa, son las que **más parametros tienen**, y por consiguiente, las que **más memoria ocupan**.\n",
        "\n",
        "Luego tambien tenemos capas convolucionales, de pooling, de dropout, y de muchos otros tamaños. Así que en la práctica, es difícil calcular a mano cual es el número máximo de muestras que podemos usar.\n",
        "\n",
        "Lo que se hace es probar con tamaños de batch potencia de 2 e ir disminuyéndolo si tenemos un error de memoria. Por ejemplo, podríamos empezar con 512, y si nos da error ir bajando a 256, 128, 64, 32, 16, 8, 4, 2 e incluso 1. Dependiendo de la arquitectura de nuestra red, puede llegar a pasarnos que tengamos que usar $k=1$, y por tanto, SGD.\n",
        "\n",
        "Aunque muchas veces es preferible disminuir el tamaño de la imagen, por ejemplo, de 512x512 a 256x256 o 128x128 pixels, y usar un $k$ mayor.\n",
        "\n",
        "Yo por ejemplo he tenido que usar $k=1$ con imágenes de 512x512 y la arquitectura DenseNet. **Y no hay problema**, simplemente tarda **más tiempo** en llegar a una solución adecuada.\n",
        "\n",
        "Vamos a ver algunos casos de diferentes bath size siguiendo con el ejemplo anterior:"
      ]
    },
    {
      "metadata": {
        "id": "uZLOQ3nTO_bj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ejemplos batch size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BeCdwKLGPkAy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Es muy importante tener en cuenta que el **learning rate va relacionado con el batch size**.\n",
        "\n",
        "Si nos aproximamos a $k=1$, debemos bajar el learning rate para que las actualizaciones de los pesos tengan menos importancia, ya que cada vez se aproxima más al SGD, es decir, a cálculos del gradiente con muestras aleatorias. \n",
        "\n",
        "Veamos qué pasa si disminuimos el learning rate:"
      ]
    },
    {
      "metadata": {
        "id": "Mm2a0cHOQFS0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# pruebas jugando con el learning rate y el batch size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6piKxddOQWvL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Y si os fijáis en la curva de pérdidas, podéis intuir que todavía es demasiado grande, puesto que la función no es monótona decreciente, es decir, que no disminuye en cada época. Así que sería mejor usar un learning rate aun menor. Veámoslo:"
      ]
    },
    {
      "metadata": {
        "id": "geMndMZoD4PV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Funciones de pérdidas y funciones de activación\n",
        "\n",
        "### 3.3.1 Funciones de pérdidas\n",
        "\n",
        "¿Qué es una función de pérdidas (o de coste)? ¿Quién sabe decírmelo?\n",
        "\n",
        "La función de pérdidas es la que nos indica en cuánto nos hemos equivocado con nuestras predicciones.\n",
        "\n",
        "Imaginad que tenemos que adivinar cuánto cuesta una casa simplemente viendo una foto. Nuestra red tendría como entrada los píxels de la foto, y como salida un número indicando el precio.\n",
        "\n",
        "Por ejemplo, digamos que queremos predecir el precio de esta casa:\n",
        "\n",
        "<img src=\"https://cdn.elgrupoinformatico.com/img/w720/Noticias/2018/05/memes-chalet-pablo-iglesias-irene-montero-720x362.jpg\" border=\"0\">\n",
        "\n",
        "Imaginaos que estamos entrenando la red y que esta casa está dentro de nuestro conjunto de training. Cuando la imagen pasa hacia delante, se calcula una predicción, que es que vale 323.567€. La verdad es que la casa cuesta 600.000€, así que parece obvio que una función de pérdidas adecuada podría ser:\n",
        "\n",
        "$f_{loss} = prediccion - valor\\_real$\n",
        "\n",
        "¿Lo entendéis verdad? Es exactamente lo mismo que con un SVM o la regresión lineal, por ejemplo.\n",
        "\n",
        "Bueno, pues para no enrollarme mucho, estas son las funciones de pérdidas más comunes:\n",
        "\n",
        "* **Problemas de regresión**\n",
        " * Mean Squared Error\n",
        " * Mean Absolute Error\n",
        "* **Problemas de clasificación**\n",
        " * Binary Cross-Entropy\n",
        " * Categorical Cross-Entropy\n",
        "\n",
        "Veamos qué son cada una:\n",
        "\n",
        "* Mean Squared Error\n",
        "\n",
        "$$MSE = \\frac{1}{n} \\sum^{n}_{i=1} (y_i-x_i)^2$$\n",
        "\n",
        "* Mean Absolute Error\n",
        "\n",
        "$$MAE = \\frac{1}{n} \\sum^{n}_{i=1} |y_i-x_i|$$\n",
        "\n",
        "Hasta aquí todo bien verdad? Pero.. qué es la entropía cruzada o cross-entropy?\n",
        "\n",
        "Vamos a verlo en el notebook de la Cros-entropía.\n",
        "\n",
        "Bueno, pues ahora que está claro qué son las funciones de pérdidas, vamos a por las de activación!\n",
        "\n",
        "### 3.3.2 Funciones de activación\n",
        "\n",
        "Os he de confesar una cosa: la magia de las redes neuronales no es solo el back-propagation. Sin las funciones de activación, las redes neuronales no funcionarían. ¿Alguien sabe decirme por qué?\n",
        "\n",
        "..\n",
        "\n",
        "\n",
        "..\n",
        "\n",
        "\n",
        "..\n",
        "\n",
        "Os doy una pista:\n",
        "\n",
        "### Qué pasaría si no existiese la función de activación?\n",
        "\n",
        "<img src=\"https://image.ibb.co/ftYCVo/what_if_no_activation_function.png\" alt=\"what_if_no_activation_function\" border=\"0\" width=\"600\">\n",
        "\n",
        "Tendríamos que $y(x)= Wx + b$. Esto es una combinación lineal que sería incapaz incluso de resolver un problema como el XOR.\n",
        "\n",
        "<img src=\"https://image.ibb.co/kg9GO8/xor.gif\" alt=\"xor\" border=\"0\">\n",
        "\n",
        "Por lo tanto, necesitamos una forma de introducir **no linealidades**, y de eso es de lo que se encarga la **función de activación**. En la siguiente imagen podéis ver algunas de las más típicas, y donde interviene en la red:\n",
        "\n",
        "<img src=\"https://image.ibb.co/bVS6qo/perceptron_activation.png\" alt=\"perceptron_activation\" border=\"0\" width=\"600\">\n",
        "\n",
        "Aquí podéis ver las más usadas:\n",
        "\n",
        "<img src=\"https://image.ibb.co/gMG5kd/activation_functions.png\" alt=\"activation_functions\" border=\"0\" width=\"600\">\n",
        "\n",
        "Es difícil saber con cual de ellas nuestra red se va a comportar mejor, pero existe una que suele dar buenos resultados casi siempre: la **ReLU**. Por lo tanto, siempre que empecemos, emplearemos la ReLU, y una vez consigamos unos resultados que consideremos buenos, podemos probar con la Leaky ReLU, o cualquier otra que os apetezca. Cada día salen nuevas, y una simple búsqueda en google os puede llevar a alguna interesante: como la SELU, por ejemplo (https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9).\n",
        "\n",
        "Muchas de estas funciones de activación necesitan métodos específicos de **inicialización de pesos**, para que esten dentro de unos valores y que el descenso del gradiente funcione adecuadamente.\n",
        "\n",
        "Veamos qué es lo que pasa si empleamos diferentes funciones de activación con nuestra ya conocida red:"
      ]
    },
    {
      "metadata": {
        "id": "9LUQxK5nVrKo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ejemplos de funciones de activación"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rc8JoxsdHhrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vamos a probar con la Sigmoide\n",
        "\n",
        "<img src=\"https://image.ibb.co/cMMqJ8/act_fn_sigmoid.png\" alt=\"act_fn_sigmoid\" border=\"0\" height=\"250\">"
      ]
    },
    {
      "metadata": {
        "id": "2ncItKPAyimS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la sigmoide"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHiIKghWHz3f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Veamos con la tanh\n",
        "\n",
        "<img src=\"https://image.ibb.co/haCKBT/act_fn_tanh.png\" alt=\"act_fn_tanh\" border=\"0\" height=\"250\">"
      ]
    },
    {
      "metadata": {
        "id": "a_wY_6hiXIyD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la tanh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3tSe_xXKKJh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ¡Vaya! ¿Qué está pasando?\n",
        "\n",
        "Probemos a cambiar la inicialización de la bias, en vez de a 0s, a 1s."
      ]
    },
    {
      "metadata": {
        "id": "yGYKMiT1KdLL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la tanh y bias 1 en vez de 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dAJF2Jb6K1He",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parece que hemos conseguido mejorarlo!!\n",
        "\n",
        "** Ahora, por lo menos, nuestra red se entrena... pero no se comporta todo lo bien que esperábamos, verdad?**\n",
        "\n",
        "Vamos a probar a cambiar la función de pérdidas, de la cros-entropía al error cuadrático medio:"
      ]
    },
    {
      "metadata": {
        "id": "l0TBurcgK9DB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la tanh, bias = 1 y loss_fn = mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wlKmAv_wMFwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "¿Os dáis cuenta? Hay 3 cosas que van íntimamente ligadas y de las que debemos ocuparnos para encontrar una buena configuración:\n",
        "\n",
        "* **La función de activación**\n",
        "* **La función de pérdidas**\n",
        "* **La inicialización de los pesos y bias**"
      ]
    },
    {
      "metadata": {
        "id": "YB9r83MkMnCn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la tanh, weights = 1, bias = 1 y loss_fn = mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dpi7Ekg_OeJo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Y si inicializamos los pesos aleatoriamente escogiendolos de una distribución normal *truncada*?"
      ]
    },
    {
      "metadata": {
        "id": "99i1G7F5PJp3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la tanh, weights = 1, bias = 1 y loss_fn = mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iWgDJYQxM1s5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lo que pretendo con estos ejemplos es que os deis cuenta de la importancia de estos 3 parámetros. Recordad, merece la pena *perder* un poco de tiempo hasta encontrar una configuración adecuada de:\n",
        "\n",
        "* **La función de activación**\n",
        "* **La función de pérdidas**\n",
        "* **La inicialización de los pesos y bias**\n",
        "\n",
        "### También es importante que tengáis en cuenta que ahora mismo, nuestra red tiene una sola capa. Cuando tengamos redes con varias capas ocultas, hay funciones de activación que se deben utilizar en las capas ocultas y otras en las de salida. Por ejemplo, las de tipo ReLU, ELU y Maxout, son para las capas ocultas. La softmax es la que se suele emplear siempre en la salida, ya que da una distribución de probabilidades que suman 1.\n",
        "\n",
        "Vamos a seguir con las otras funciones de activación. Usaremos la red que usa el MSE y una bias = 1."
      ]
    },
    {
      "metadata": {
        "id": "KHnklfiKIBvC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Veamos con la ReLU\n",
        "\n",
        "<img src=\"https://image.ibb.co/mSsKBT/act_fn_relu.png\" alt=\"act_fn_relu\" border=\"0\" height=\"250\">\n",
        "\n",
        "<img src=\"https://image.ibb.co/iUdsWT/act_fn_relu2.png\" alt=\"act_fn_relu2\" border=\"0\" height=\"250\">\n",
        "\n",
        "<img src=\"https://image.ibb.co/fzXako/act_fn_relu3.png\" alt=\"act_fn_relu3\" border=\"0\" height=\"250\">"
      ]
    },
    {
      "metadata": {
        "id": "omYn8ADoIYII",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la ReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-mYTORMJIZqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Veamos con la Leaky RELU\n",
        "\n",
        "<img src=\"https://image.ibb.co/j5pAJ8/act_fn_lrelu.png\" alt=\"act_fn_lrelu\" border=\"0\" height=\"250\">"
      ]
    },
    {
      "metadata": {
        "id": "F0_60T2MIhQQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la Leaky ReLU, bias = 1 y loss_fn = mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfyGWtw8Ih_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Veamos con la ELU\n",
        "\n",
        "<img src=\"https://image.ibb.co/gEJgQo/act_fn_elu.png\" alt=\"act_fn_elu\" border=\"0\" height=\"250\">"
      ]
    },
    {
      "metadata": {
        "id": "jN9Zx6ylIozk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# probamos la ELU, bias = 1 y loss_fn = mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3tgMjOAIsMh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Veamos con la Maxout\n",
        "\n",
        "<img src=\"https://image.ibb.co/hWg1Qo/act_fn_maxout.png\" alt=\"act_fn_maxout\" border=\"0\" height=\"250\">\n",
        "\n",
        "Básicamente esta función de activación devuelve el máximo de múltiples posibles salidas para cada entrada. Al final, es una generalización de la ReLU y la Leaky ReLU. Para la ReLU, por ejemplo, tenemos $w_1=b_1=0$). Por lo tanto, la Maxout se beneficia de todas las ventajas de la ReLU (es lineal y no satura) y además evita matar el gradiente cuando $x<0$.\n",
        "\n",
        "Para que nos entendamos, podemos aproximar cualquier función con varias maxouts, como podéis ver en la imagen siguiente, en la que aproximamos $y=x^2$ con 3 maxouts:\n",
        "\n",
        "<img src=\"https://image.ibb.co/co951T/maxout_aproximator.png\" alt=\"maxout_aproximator\" border=\"0\" height=\"250\">\n",
        "\n",
        "Este tipo de activación funciona bien cuando se combina con Dropout, que lo veremos al final de esta sesión.\n",
        "\n",
        "No os preocupéis, más adelante implementaremos nuestra función de activación Maxout! ;-)\n",
        "\n",
        "Si queréis saber más, aquí tenéis una presentación de Ian Goodfellow donde explican cómo funciona:  http://www.ms.uky.edu/~qye/MA721/maxout.pdf\n",
        "\n",
        "Y aquí el paper original: https://arxiv.org/abs/1302.4389\n",
        "\n",
        "Más info: \n",
        "\n",
        " * http://cs231n.github.io/neural-networks-1/"
      ]
    },
    {
      "metadata": {
        "id": "AL9rwNH1I5I1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Atención:\n",
        "\n",
        "### Como he comentado antes, en el caso de las capas de salida, practicamente siempre se utiliza la función de activación de tipo softmax, ya que es capaz de dar una probabilidad a cada clase, haciendo que todas ellas sumen 1.\n",
        "\n",
        "Como esto puede parecer un poco complicado, os voy a escribir aquí la receta que sigo yo y que hasta ahora me ha dado buenos resultados:\n",
        "\n",
        "# Receta\n",
        "\n",
        "* Empezar usando la ReLU con un learning rate de 0.01 o 0.001, y observar qué pasa.\n",
        "\n",
        "* Si la red entrena (va convergiendo) pero es lenta, podéis probar a aumentar un poco el learning rate\n",
        "\n",
        "* Si la red no converge y se compora de forma caótica, disminuid el learning rate\n",
        "\n",
        "* Una vez tengáis la red funcionando, probad con la Leaky ReLU, la Maxout o la ELU\n",
        "\n",
        "* No uséis la sigmoide, en la práctica no suele dar buenos resultados\n"
      ]
    },
    {
      "metadata": {
        "id": "2pxfMOpR2WEz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.5 Inicialización de pesos\n",
        "\n",
        "Como habéis visto antes, la inicialización de los pesos y la bias es muy importante para conseguir la convergencia de nuestra red a un mínimo adecuado. Así que vamos a ver algunas formas de inicializar los pesos.\n",
        "\n",
        "Siguiendo con nuestro ejemplo del MNIST, nuestra matriz de pesos va a ser de 768 (entradas) x 10 (salidas).\n",
        "\n",
        "### 3.5.1 Inicialización constante\n",
        "\n",
        "Podemos inicializar nuestros pesos a cero:\n",
        "\n",
        "`W = np.zeros((768, 10))`\n",
        "\n",
        "A uno:\n",
        "\n",
        "`W = np.ones((768, 10))`\n",
        "\n",
        "O a una constante $C$:\n",
        "\n",
        "`W = np.ones((768, 10)) * C`\n",
        "\n",
        "### 3.5.2 Distribución normal y uniforme\n",
        "\n",
        "También podemos inicializar los pesos usando una distribución **uniforme**, en la que se definen un $[upper\\_bound, lower\\_bound]$ y todos los números dentro del rango tienen la misma probabilidad de ser escogidos. Por ejemplo, para una distribución entre $[-0.2, 0.2]$:\n",
        "\n",
        "`W = np.random.uniform(low=-0.2, high=0.2, size=(768, 10))`\n",
        "\n",
        "Con esta instrucción, inicializaremos la matriz de pesos $W$ con valores extraídos del rango entre $[-0.2, 0.2]$ donde todos ellos tienen la misma probabilidad de ser extraídos.\n",
        "\n",
        "También podemos hacerlo con una distribución **normal** o Gaussiana, la cual viene definida como:\n",
        "\n",
        "$$p(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
        "\n",
        "Donde como ya sabéis:\n",
        "\n",
        "* $\\mu$ es la media\n",
        "* $\\sigma$ es la desviación estándar, y $\\sigma^2$ la varianza\n",
        "\n",
        "Así que podríamos inicializar nuestros pesos con una distribución normal con $\\mu = 0$ y $\\sigma = 0.2$, por ejemplo, de la siguiente forma:\n",
        "\n",
        "`W = np.random.normal(0.0, 0.2, size=(768, 10))`\n",
        "\n",
        "### 3.5.3 Inicialización: LeCun normal y uniforme\n",
        "\n",
        "Otra forma más avanzada es el método de LeCun, también conocido como *\"Efficient backprop\"*.\n",
        "\n",
        "Este método define 3 parámetros:\n",
        "\n",
        "* $f_{in}$: número de entradas a la capa (en nuestro ejemplo, 768)\n",
        "* $f_{out}$: número de salidas de la capa (en nuestro ejemplo, 10)\n",
        "* $limit$: queda definido según $f_{in}$ y $f{out}$ como $limit = \\sqrt{\\frac{3}{f_{in}}}$\n",
        "\n",
        "El código para inicializar $W$ mediante este método usando una distribución uniforme sería:\n",
        "\n",
        "`W = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))`\n",
        "\n",
        "Y para una normal:\n",
        "\n",
        "`W = np.random.normal(low=-limit, high=limit, size=(F_in, F_out))`\n",
        "\n",
        "### 3.5.4 Inicialización: Glorot/Xavier normal y uniforme\n",
        "\n",
        "Esta es quizás el método más empleado a la hora de inicializar los pesos y la bias. De hecho, es el empleado por defecto cuando se utiliza Keras, un framework que veréis en la próxima sesión.\n",
        "\n",
        "En este caso también se definen los mismos parámetros que con LeCun, pero varía el cálculo del $limit$:\n",
        "\n",
        " $limit = \\sqrt{\\frac{2}{f_{in}+f_{out}}}$\n",
        " \n",
        " El código para inicializar $W$ mediante este método sería el mismo que con LeCun. \n",
        " \n",
        " Para una distribución uniforme sería:\n",
        "\n",
        "`W = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))`\n",
        "\n",
        "Y para una normal:\n",
        "\n",
        "`W = np.random.normal(low=-limit, high=limit, size=(F_in, F_out))`\n",
        "\n",
        "### 3.5.5 Inicialización: He et al./Kaiming/MSRA normal y uniforme\n",
        "\n",
        "Este método debe su nombre a Kaiming He, el primer autor de *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*.\n",
        "\n",
        "Normalmente, este método se usa cuando estamos entrenando redes neuronales *muy profundas* que usan un tipo particular de ReLU como activación: la Parametric ReLU).\n",
        "\n",
        "El código en el caso de la uniforme es este:\n",
        "\n",
        "`limit = np.sqrt(6 / float(F_ini))`\n",
        "\n",
        "`W = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))`\n",
        "\n",
        "\n",
        "Y en el caso de la normal, este:\n",
        "\n",
        "\n",
        "`limit = np.sqrt(2 / float(F_ini))`\n",
        "\n",
        "`W = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))`\n",
        "\n",
        "\n",
        "## Consejo:\n",
        "\n",
        "La inicialización de los pesos no suele ser determinante en el entrenamiento de una red, pero a veces puede hacer que la red no empiece a entrenar porque no consigue converger. Por lo tanto, mi consejo es que uséis la de Glorot, y si ese día estáis aventureros y queréis ver si conseguís hacer que mejore la precisión, probéis con otras ;-)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "owGPgmwnBlvu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}