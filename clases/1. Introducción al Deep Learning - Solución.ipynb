{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJQZDOSRlvjY"
   },
   "source": [
    "# 1. Deep Learning\n",
    "\n",
    "Bienvenidos al módulo de Deep Learning. En esta sesión vamos a ver una breve introducción con las diferencias entre Inteligencia Artificial, Machine Learning y Deep Learning. También hablaremos un poco de cuanto tiempo lleva este mundillo en marcha, e introduciremos el gradient descent, el pilar fundamental de las redes neuronales.\n",
    "\n",
    "En la segunda parte empezaremos a trabajar con TensorFlow, y al terminar, habréis implementado vuestra **primera red neuronal**!\n",
    "\n",
    "Aquí tenéis el temario que cubriremos hoy:\n",
    "\n",
    "**Introducción**\n",
    "* Del Machine Learning al Deep Learning\n",
    "* Contexto histórico\n",
    "* Herramienta: Google Colab\n",
    "* Descenso del gradiente\n",
    "\n",
    "**Introducción a TensorFlow**\n",
    "* Grafos, variables, operaciones\n",
    "* Resolución de problemas con TensorFlow\n",
    "* Mi primera red neuronal en TensorFlow\n",
    "\n",
    "¿Qué os parece? ¿Tenéis ganas?\n",
    "\n",
    "**¡Vamos allá!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0dQrH-TDD1z"
   },
   "source": [
    "## 1.1. Introducción. Del Machine Learning al Deep Learning.\n",
    "\n",
    "¿Qué es el Deep Learning? ¿En qué se diferencia del Machine Learning? ¿Y de la Inteligencia Artificial?\n",
    "\n",
    "<a href=\"https://ibb.co/hWqjG7\"><img src=\"https://preview.ibb.co/cETFOn/AI_ML_DL.png\" alt=\"IA engloba ML, que a su vez engloba DL\" border=\"0\"></a>\n",
    "\n",
    "**Inteligencia artificial:** \n",
    "\n",
    "La IA consiste en conseguir que las máquinas realicen tareas que requieren de la inteligencia humana. Se puede dividir en dos campos:\n",
    "\n",
    "*   *General AI*: consiste en dotar a las máquinas de todas nuestras capacidades y sentidos. Por ejemplo, C-3PO o Terminator. Tranquilos, todavía queda muuuucho para ver algo parecido a Skynet por aquí.\n",
    "*   *Narrow AI*: consiste en dotar a las máquinas de la capacidad de desarrollar una determinada tarea, como por ejemplo, reconocer caras, señales de tráfico, el habla, etc. Es en este campo donde actualmente se están observando grandes avances.\n",
    "\n",
    "**Machine Learning:**\n",
    "\n",
    "El Machine Learning o aprendizaje máquina es un **subcampo dentro de la Inteligencia Artificial**. Básicamente, consiste en **utilizar una gran cantidad de datos para extraer información útil para las personas**. Por ejemplo, imaginaos que disponemos de los últimos resultados de La Liga y queremos ser capaces de predecir el resultado del próximo Clásico. Primero, necesitariamos *parsear* los datos, limpiarlos, eliminar las entradas incompletas, estudiar la distribución de las variables y elegir las características o atributos (*feature engineering*) que nos permitieran predecir este resultado con la mayor precisión posible. Una vez tuviésemos claro cuales son las mejores características a utilizar, necesitaríamos encontrar el mejor algoritmo posible para nuestro dataset.\n",
    "\n",
    "La **elección de las características**, que a priori puede llegar a parecer sencillo, es el paso **más complicado y costoso** y, además, requiere de un alto grado de conocimientos sobre el problema en cuestión y sobre técnicas de extracción de características.\n",
    "\n",
    "**Deep Learning:**\n",
    "\n",
    "El aprendizaje profundo o Deep Learning es un subcampo del **Machine Learning**, y soluciona el problema anterior de la elección de características. También llamado aprendizaje jerárquico, ***aprende*** distintas **representaciones de los datos** que son introducidas a un clasificador final.\n",
    "\n",
    "La magia está en que ya no necesitamos volvernos locos buscando las mejores características o atributos para cada problema, si no que esto lo hace **automáticamente** nuestro algoritmo.\n",
    "\n",
    "\n",
    "Por último, cabe destacar la importancia de tener una gran cantidad de datos para poder utilizar técnicas de ML o DL. Además, cuanta más calidad tengan esos datos, mejor se comportaran nuestros modelos.\n",
    "\n",
    "*“I think AI is akin to building a rocket ship. You need a huge engine and a lot of fuel. If you have a large engine and a tiny amount of fuel, you won’t make it to orbit. If you have a tiny engine and a ton of fuel, you can’t even lift off. To build a rocket you need a huge engine and a lot of fuel.”*\n",
    "\n",
    "– Andrew Ng (source: Wired)\n",
    "\n",
    "Que traducido quiere decir:\n",
    "\n",
    "*Creo que la IA es parecido a construir un cohete espacial. Necesitas un motor enorme y mucho combustible. Si tienes un motor enorme pero poco combustible, no conseguirás poner el cohete en órbita. Si tienes un pequeño motor y un montón de combustible, no podrás ni despegar. Para construir un cohete espacial necesitas un motor enorme y un monton de combustible.*\n",
    "\n",
    "Así que ya sabéis, no solo es importante el algoritmo que utilicemos, sino los datos de los que dispongamos.\n",
    "\n",
    "¿Y por qué **deep** learning?\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/wide_n_deep.svg\" border=\"0\" height=\"300\">\n",
    "\n",
    "Porque realizamos muchas transformaciones a los datos que tenemos, una detrás de la otra, de una forma secuencial, hasta que llegamos a la representación de esos datos en la que somos capaces de diferenciar los datos conforme nosotros queremos.\n",
    "\n",
    "Aquí tenéis un muy buen ejercicio que podéis hacer cuando tengáis un rato: https://www.tensorflow.org/tutorials/wide_and_deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFDhMkz78NoB"
   },
   "source": [
    "### 1.1.1. Contexto histórico\n",
    "\n",
    "Probablemente muchos de vosotros penséis que esto del Deep Learning y las redes neuronales es una novedad, pues nada más lejos de la realidad! De hecho, la primera red neuronal data de 1958!! Lo que pasa es que no había potencia de cálculo suficiente para poder progresar hasta hace relativamente poco, con la popularización de las GPUs de altas prestaciones. Así que... gracias gamers!!\n",
    "\n",
    "Por si tenéis curiosidad, estas son las cosas más importantes que han ido pasando dentro del campo de la IA:\n",
    "\n",
    "<img src=\"https://image.ibb.co/mEVbA8/timeline_deep_learning.png\" alt=\"timeline_deep_learning\" border=\"0\">\n",
    "\n",
    "*   **1950**: Alan Turing crea el **Test de Turing**\n",
    "*   **1952**: Arthur Samuel crea el primer programa que **aprende** partida tras partida a jugar a las Damas\n",
    "*   **1956**: Martin Minsky acuña el término **\"Artificial Intelligence\"** para referirse a este nuevo campo\n",
    "*   **1958**: Frank Rosenblatt diseña el Perceptrón, **la primera red neuronal artificial**. HACE **60 AÑOS!!!**\n",
    "\n",
    "Como podéis ver, la Inteligencia Artificial no es ni mucho menos nueva, lleva mucho tiempo entre nosotros, sufriendo altibajos debidos a las altas expectativas depositadas y los \"pocos\" avances conseguidos.\n",
    "\n",
    "Desde 1974 hasta nuestros días, se han producido varios \"inviernos\" y \"primaveras\".\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/b3ih3n/AI_winter.jpg\" alt=\"Winter is coming\" border=\"0\"></a>\n",
    "\n",
    "Algunos de los hechos más relevantes hasta 2006 son estos:\n",
    "\n",
    "*   **1967**: Nace el campo del reconocimiento de patrones con la aparición del algoritmo **\"Nearest Neighbor\"**\n",
    "*   **1979**: Crean el Stanford Cart, un **robot capaz de navegar automáticamente por una habitación evitando obstáculos**\n",
    "*   **1981**: Gerald Dejong introduce el \"Explanation Based Learning\", el precursor del Machine Learning. El ordenador **analizaba datos** de entrenamiento y creaba reglas **para descartar los datos menos importantes**\n",
    "*   **1985**: Terry Sejnowski inventa NetTalk, un algoritmo capaz de **aprender a pronunciar palabras** como lo haría un niño\n",
    "*   **1990s**: **Cambia el paradigma del Machine Learning**: de un enfoque orientado al conocimiento, a uno **orientado al dato**. Empiezan a extraer conclusiones de grandes cantidades de datos.\n",
    "*   **1997**: **DeepBlue derrota a Kaspárov** por primera vez.\n",
    "\n",
    "Tras esta época de luces y sombras, y gracias, entre otras cosas, a la disponibilidad de mucha más potencia de cálculo y de datos, desde 2006 hasta la fecha ha habido una explosión del Machine Learning.\n",
    "\n",
    "*   **2006**: Aparecen las **arquitecturas profundas**, acuñadas como Deep Learning por Geoffrey Hinton\n",
    "*   **2011**: **Watson IBM vence** a sus competidores **humanos** en el concurso Jeopardy\n",
    "*   **2012**: Geoffrey Hinton gana por goleada el concurso de **ImageNet** con una red neuronal profunda\n",
    "*   **2012**: En Google X crean GoogleBrain, capaz de **detectar gatos en videos**\n",
    "*   **2014**: Facebook desarrolla DeepFace, capaz de **reconocer caras de humanos con una precisión de 97.25%**, solo un 0.28% por debajo de un ser humano\n",
    "*   **2014**: Google compra DeepMind, una stertup inglesa que había creado un algoritmo capaz de **aprender a jugar con juegos Atari simplemente viendo el video**\n",
    "*   **2015**: Elon Musk y Sam Altman crean OpenAI para **promover el bueno uso de la IA**\n",
    "*   **2016**: **Google DeepMind vence** por primera vez en el **juego Go, consiguiendo movimientos creativos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLJb8m8c8TSH"
   },
   "source": [
    "### 1.1.2. Google Colab\n",
    "\n",
    "La plataforma en la que os encontráis se llama **Google Colab**. Va a ser a la vez nuestro *notebook* y nuestra IDE, ya que nos permite aprovechar que por el momento Google ofrece GPUs **NVIDIA K80** de forma **gratuita** (maravilloso, no?), que es donde ejecutaremos nuestros desarrollos.\n",
    "\n",
    "Cualquiera con una cuenta de Google puede crear un nuevo notebook y empezar a disfrutarlo! A lo largo del curso iremos viendo lo necesario para que aprendáis a manejarlo con soltura, así que no tenéis por qué preocuparos si es la primera vez que lo véis. Y si habéis trabajado antes con Jupyter, esto es lo mismo!\n",
    "\n",
    "Lo único que tenéis que hacer todos para aprovechar la GPU es ir a **Edit**, en la barra de herramientas, y luego a **Notebook settings**. Os aparecerá esta pantalla, la cual tiene que quedar igual que en la imagen. Debéis seleccionar Python 3 y GPU, y luego darle a **Save**.\n",
    "\n",
    "<a href=\"https://ibb.co/ceoX3n\"><img src=\"https://preview.ibb.co/mQEi9S/notebook_config.png\" alt=\"Notebook configuration\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i82s0YnP8XPH"
   },
   "source": [
    "### 1.1.3. El descenso del gradiente: el culpable de que existan las redes neuronales\n",
    "\n",
    "El mecanismo que hace que una red neuronal aprenda es el descenso del gradiente. Como posiblemente ya sabréis los más curiosos, las redes neuronales se entrenan actualizando una serie de pesos, y conforme más se entrenan, más se acercan a una buena solución. ¿Os suena esto de algo?\n",
    "\n",
    "\n",
    "¿No?\n",
    "\n",
    "\n",
    "¡Estamos optimizando funciones! En efecto, siento desilusionaros, pero las redes neuronales no son otra cosa que un método, muy ingenioso y potente, eso sí, de **optimizar funciones**. Y nuestro mejor amigo se llama **Descenso del gradiente**, que es lo que en gran medida ha permitido que podamos entrenar redes con millones de parámetros y vivir para verlo!\n",
    "\n",
    "**¿Pero qué es el descenso del gradiente?**\n",
    "\n",
    "Vamos a imaginarnos que estamos en lo alto de una montaña y que hay una niebla tremenda, con lo cual no somos capaces de ver más allá de nuestras narices, literalmente. Lo que queremos lógicamente es llegar abajo del todo, donde hay un bar estupendo con una cervecita bien fría esperándonos. Aunque más de uno puede que haya saltado ya y no le importe bajar rodando, para aquellos más cautos, os voy a explicar cómo lo vamos a hacer. Poneos en situación. Estáis en el punto más alto y no veis nada, ¿cómo lo hacéis para bajar la montaña?\n",
    "\n",
    "<img src=\"https://image.ibb.co/jhnVbJ/mountain.png\" alt=\"mountain\" border=\"0\" height=\"300\">\n",
    "\n",
    "¿Ningún valiente? Ya he dicho que rodando no nos vale.\n",
    "\n",
    "¿Y si damos un pequeño paso en cada una de las 4 direcciones (este, oeste, norte y sur) y elegimos aquella en la que más bajamos para dar un paso? ¿Y si volvemos a realizar esta operación una y otra vez? Podríamos hacer esto hasta que no consiguieramos bajar en ninguna de las 4 posibles orientaciones. Entonces, en teoría, deberíamos haber llegado abajo.\n",
    "\n",
    "¿Qué? ¿No os termina de convencer verdad? Eso de dar 4 pasos a ver cual es mejor... no pareece muy eficiente ¿verdad? ¿Se os ocurre alguna forma de mejorar esto? Ojalá hubiese alguna forma de saber qué pendiente hay en cada punto, no? eso nos permitiría avanzar siempre hacia donde más pendiente hubiera.\n",
    "\n",
    "¿Os suena esto de algo? ¿Cómo podemos averiguar la pendiente de una función?\n",
    "\n",
    "¡Eso es! ¡Calculando la **derivada**!\n",
    "\n",
    "¿Os acordáis de qué es lo que indica la derivada de una función? ¿Os acordáis de cómo se calculan?\n",
    "\n",
    "$f'(x) = \\frac{f(x+h)-f(x-h)}{2}$, cuando $h\\to 0$.\n",
    "\n",
    "La primera derivada de una función mide la rapidez con que cambia una función, es decir cuanto crece o decrece. Así que lo que podemos hacer es **calcular la pendiente para cada punto al que llegamos seguir bajando esa pendiente hasta un punto mínimo**.\n",
    "\n",
    "<img src=\"https://image.ibb.co/cXGCqd/mountain_gd.png\" alt=\"mountain_gd\" border=\"0\" height=\"300\"> <img src=\"https://image.ibb.co/ganZLd/gradient_descent_2.png\" alt=\"gradient_descent_2\" border=\"0\" height=\"300\">\n",
    "\n",
    "Pues si amigos, esto taaaan sofisticado es el algoritmo del descenso del gradiente, y os puedo decir que es **el corazón de las redes neuronales**.\n",
    "\n",
    "Para aquellos de vosotros que os gusten más las matemáticas que las montañas, podéis ver el descenso del gradiente como un algoritmo de optimización que permite **minimizar cualquier función** (siempre que sea **diferenciable**, es decir, que podamos calcular sus derivadas). De hecho, la idea es muy similar a aquellos problemas de optimización a los que muy posiblemente os hayáis enfrentado en alguna clase de matemáticas a lo largo de vuestras vidas. Y para muestra, un botón:\n",
    "\n",
    "<center><img src=\"http://www.cs.us.es/~fsancho/images/2017-02/gradient_descent.gif\" border=\"0\"></center>\n",
    "\n",
    "Este algoritmo tiene variaciones (vanilla gradient descent, batch gradient descent, stochastic gradient descent, etc), pero todos parten de la misma idea base, la que os acabo de explicar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcokI_bpc4Ui"
   },
   "source": [
    "## 1.2. Introducción a TensorFlow\n",
    "\n",
    "Como muchos de vosotros ya sabréis, TensorFlow es un framework desarrollado y mantenido por Google que permite la ejecución de operaciones matemáticas de una forma optimizada en una CPU o GPU. En nuestro caso estamos más interesados en la GPU, ya que es la única forma que tenemos de entrenar una red neuronal profunda y no morir de viejos esperando ;)\n",
    "\n",
    "¿Por qué tensorflow?\n",
    "\n",
    "* Por su **flexibilidad y escalabilidad**\n",
    "\n",
    "* Por su **popularidad**\n",
    "\n",
    "<img src=\"https://image.ibb.co/mCJNqd/tf_pop.png\" alt=\"tf_pop\" border=\"0\" height=\"300\"> <img src=\"https://image.ibb.co/bBDYwJ/tf_users.png\" alt=\"tf_users\" border=\"0\" height=\"300\">\n",
    "\n",
    "\n",
    "Más adelante comprenderemos por qué es tan importante disponer de un buen framework que nos permita realizar operaciones de la forma más rápida posible. De momento, vamos a trabajar un poco con TensorFlow hasta que nos familiaricemos con su modo de funcionar. Pero antes, algunas de sus características más importantes:\n",
    "\n",
    "*    TensorFlow utiliza **tensores** para realizar las operaciones.\n",
    "*    En TensorFlow, primero se definen las operaciones a realizar (construimos el **grafo**), y luego se ejecutan (se ejecuta el grafo)\n",
    "*    Permite ejecutar el código implementado **paralelamente** o en una o varias GPUs, a elección del usuario\n",
    "\n",
    "Vale, pero **¿qué es un tensor?**\n",
    "\n",
    "Aunque los tensores los inventaron los físicos para ser capaces de describir interacciones, en el ámbito de la IA se pueden entender simplemente como **contenedores de números**.\n",
    "\n",
    "<a href=\"https://ibb.co/bBCyb7\"><img src=\"https://image.ibb.co/fnFvOn/tensores.png\" alt=\"tensores\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1528644942797,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113131840338433251202"
     },
     "user_tz": -60
    },
    "id": "dIIqtgJf3P_5",
    "outputId": "a53add81-bb89-4904-e381-0b5f41b39f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nota media:\n",
      "5\n",
      "Dimensiones del tensor: 0\n"
     ]
    }
   ],
   "source": [
    "# Vamos a practicar con nuestros nuevos mejores amigos, los Tensores\n",
    "import numpy as np\n",
    "\n",
    "# Imaginaos que queréis guardar la nota media del máster de Tristina Cipuentes.\n",
    "# Para ello utilizaríais un tensor de 0D, que no es otra cosa que un simple \n",
    "# número, también conocido como escalar.\n",
    "\n",
    "# Tensor 0D (escalar)\n",
    "tensor_0D = np.array(5)\n",
    "print(\"Nota media:\\n{}\".format(tensor_0D))\n",
    "print(\"Dimensiones del tensor: {}\".format(tensor_0D.ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1528644945021,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113131840338433251202"
     },
     "user_tz": -60
    },
    "id": "zkk4unVy4CwS",
    "outputId": "3ef8968b-7aaf-4405-b903-60ca74e29dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notas de las asignaturas:\n",
      "[2 8 6]\n",
      "Dimensiones del tensor: 1\n"
     ]
    }
   ],
   "source": [
    "# Pero con una sola nota no habría mucho que defender, podría decirse incluso\n",
    "# que alguien se la ha inventado, así que mejor si guardamos las notas de\n",
    "# TODOS las asignaturas que hizo. Podemos usar un tensor 1D para ello:\n",
    "\n",
    "# Tensor 1D (vector)\n",
    "tensor_1D = np.array([2, 8, 6])\n",
    "print(\"Notas de las asignaturas:\\n{}\".format(tensor_1D))\n",
    "print(\"Dimensiones del tensor: {}\".format(tensor_1D.ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1528644946147,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113131840338433251202"
     },
     "user_tz": -60
    },
    "id": "_ab-ex3W4tfc",
    "outputId": "fa428755-5dae-46d9-9c27-68697e34ac5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las puntuaciones de Tristina en sus exámenes son:\n",
      "[[0 1 1]\n",
      " [2 3 3]\n",
      " [1 3 2]]\n",
      "Asignatura 1:\n",
      "[0 1 1]\n",
      "Asignatura 2:\n",
      "[2 3 3]\n",
      "Asignatura 3:\n",
      "[1 3 2]\n",
      "Dimensiones del tensor: 2\n"
     ]
    }
   ],
   "source": [
    "# Un momento... ¿no sería mejor que nos guardásemos la puntuación de cada \n",
    "# examen de cada asignatura? ¿Cómo podríamos hacerlo, si cada asignatura consta \n",
    "# de 3 exámenes? ¿Se os ocurre alguna estructura que nos lo permita?\n",
    "\n",
    "# En efecto, un tensor 2D!\n",
    "\n",
    "# Tensor 2D (matriz)\n",
    "tensor_2D = np.array([[0, 1, 1],  # asignatura 1\n",
    "                      [2, 3, 3],  # asignatura 2\n",
    "                      [1, 3, 2]])  # asignatura 3\n",
    "print(\"Las puntuaciones de Tristina en sus exámenes son:\\n{}\".format(tensor_2D))\n",
    "\n",
    "print(\"Asignatura 1:\\n{}\".format(tensor_2D[0]))\n",
    "\n",
    "print(\"Asignatura 2:\\n{}\".format(tensor_2D[1]))\n",
    "\n",
    "print(\"Asignatura 3:\\n{}\".format(tensor_2D[2]))\n",
    "\n",
    "print(\"Dimensiones del tensor: {}\".format(tensor_2D.ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1528644947255,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113131840338433251202"
     },
     "user_tz": -60
    },
    "id": "3cZSBcPu597b",
    "outputId": "fc2fb8bf-0f98-4623-d6ba-0cc03c8e9b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las notas de Tristina por cuatrimestre son:\n",
      "[[[0 1 1]\n",
      "  [2 3 3]\n",
      "  [1 3 2]]\n",
      "\n",
      " [[1 3 2]\n",
      "  [2 4 2]\n",
      "  [0 1 1]]]\n",
      "Primer cuatrimestre:\n",
      "[[0 1 1]\n",
      " [2 3 3]\n",
      " [1 3 2]]\n",
      "Segundo cuatrimestre:\n",
      "[[1 3 2]\n",
      " [2 4 2]\n",
      " [0 1 1]]\n",
      "Dimensiones del tensor: 3\n"
     ]
    }
   ],
   "source": [
    "# Sin embargo, como nosotros somos muy ordenados y no queremos que se nos pierda\n",
    "# nada, mejor si guardamos las notas de las asignaturas (que son anuales) por \n",
    "# cuatrimestres, así será más fácil acceder a ellas si hiciese falta en un \n",
    "# futuro, ¿no os parece? ¿Cómo se os ocurre que podríamos organizarlas?\n",
    "\n",
    "# ¿Y si añadimos una dimensión a nuestro tensor 2D que indique el cuatrimestre?\n",
    "\n",
    "# Tensor 3D (matriz 3D o cubo)\n",
    "tensor_3D = np.array([[[0, 1, 1],  # Primer cuatrimestre\n",
    "                      [2, 3, 3],\n",
    "                      [1, 3, 2]],\n",
    "                     [[1, 3, 2],  # Segundo cuatrimestre\n",
    "                      [2, 4, 2],\n",
    "                      [0, 1, 1]]])\n",
    "print(\"Las notas de Tristina por cuatrimestre son:\\n{}\".format(tensor_3D))\n",
    "\n",
    "print(\"Primer cuatrimestre:\\n{}\".format(tensor_3D[0]))\n",
    "\n",
    "print(\"Segundo cuatrimestre:\\n{}\".format(tensor_3D[1]))\n",
    "\n",
    "print(\"Dimensiones del tensor: {}\".format(tensor_3D.ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1528644948273,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113131840338433251202"
     },
     "user_tz": -60
    },
    "id": "ZRztNF9K-Qx_",
    "outputId": "d771075a-f55e-40c7-be3a-283280a8e147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las notas de Tristina, Facundo y Celedonio por cuatrimestre son:\n",
      "[[[[0 1 1]\n",
      "   [2 3 3]\n",
      "   [1 3 2]]\n",
      "\n",
      "  [[1 3 2]\n",
      "   [2 4 2]\n",
      "   [0 1 1]]]\n",
      "\n",
      "\n",
      " [[[0 3 1]\n",
      "   [2 4 1]\n",
      "   [1 3 2]]\n",
      "\n",
      "  [[1 1 1]\n",
      "   [2 3 4]\n",
      "   [1 3 2]]]\n",
      "\n",
      "\n",
      " [[[2 2 4]\n",
      "   [2 1 3]\n",
      "   [0 4 2]]\n",
      "\n",
      "  [[2 4 1]\n",
      "   [2 3 0]\n",
      "   [1 3 3]]]]\n",
      "Notas de Tristina:\n",
      "[[[0 1 1]\n",
      "  [2 3 3]\n",
      "  [1 3 2]]\n",
      "\n",
      " [[1 3 2]\n",
      "  [2 4 2]\n",
      "  [0 1 1]]]\n",
      "Notas de Facundo:\n",
      "[[[0 3 1]\n",
      "  [2 4 1]\n",
      "  [1 3 2]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [2 3 4]\n",
      "  [1 3 2]]]\n",
      "Notas de Celedonio:\n",
      "[[[2 2 4]\n",
      "  [2 1 3]\n",
      "  [0 4 2]]\n",
      "\n",
      " [[2 4 1]\n",
      "  [2 3 0]\n",
      "  [1 3 3]]]\n",
      "Dimensiones del tensor: 4\n"
     ]
    }
   ],
   "source": [
    "# ¿Qué os parece? Ya tenemos perfectamente guardadas las notas de Tristina,\n",
    "# para que no se pierdan. ¿Pero qué pasa con los demás alumnos? ¿Se os ocurre\n",
    "# cómo podríamos guardarnos sus notas también?\n",
    "# ¿Y si añadimos una dimensión a nuestro tensor, de forma que podamos tener las\n",
    "# notas por cuatrimestre de cada asignatura para cada alumno?\n",
    "\n",
    "# Tensor 4D (vector de matrices 3D o vector de cubos)\n",
    "tensor_4D = np.array([[[[0, 1, 1], # Tristina\n",
    "                      [2, 3, 3],\n",
    "                      [1, 3, 2]],\n",
    "                     [[1, 3, 2],\n",
    "                      [2, 4, 2],\n",
    "                      [0, 1, 1]]],\n",
    "                      [[[0, 3, 1], # Facundo\n",
    "                      [2, 4, 1],\n",
    "                      [1, 3, 2]],\n",
    "                     [[1, 1, 1],\n",
    "                      [2, 3, 4],\n",
    "                      [1, 3, 2]]],\n",
    "                     [[[2, 2, 4], # Celedonio\n",
    "                      [2, 1, 3],\n",
    "                      [0, 4, 2]],\n",
    "                     [[2, 4, 1],\n",
    "                      [2, 3, 0],\n",
    "                      [1, 3, 3]]]])\n",
    "print(\"Las notas de Tristina, Facundo y Celedonio por cuatrimestre son:\\n{}\".format(tensor_4D))\n",
    "\n",
    "print(\"Notas de Tristina:\\n{}\".format(tensor_4D[0]))\n",
    "\n",
    "print(\"Notas de Facundo:\\n{}\".format(tensor_4D[1]))\n",
    "\n",
    "print(\"Notas de Celedonio:\\n{}\".format(tensor_4D[2]))\n",
    "\n",
    "print(\"Dimensiones del tensor: {}\".format(tensor_4D.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYU1IwoWCBaD"
   },
   "source": [
    "Y así podríamos seguir hasta el infinito añadiendo dimensiones a nuestros tensores para ser capaces de guardar más datos. Para que os hagáis una idea de cómo se suelen utilizar en el mundo del Deep Learning, los tipos más comunes de tensores son:\n",
    "\n",
    "*    **Tensores 3D**: utilizados en **series temporales**\n",
    "*    **Tensores 4D**: utilizados con **imágenes**\n",
    "*    **Tensores 5D**: utilizados con **videos**\n",
    "\n",
    "Normalmente, siempre habrá una de las dimensiones que se utilizará para almacenar las muestras de cada tipo de datos. Por ejemplo, con las imágenes:\n",
    "\n",
    "Si queremos almacenar 64 imágenes RGB de 224x224 píxels, necesitaremos un vector de matrices 3D, o lo que es lo mismo, un tensor 4D. ¿Quién sabría decirme las dimensiones que tendría nuestro vector de matrices 3D?\n",
    "\n",
    "Tenemos 64 imágenes de 224 pixels x 224 pixels x 3 canales (R, G y B). \n",
    "\n",
    "Por tanto: (64, 224, 224, 3)\n",
    "\n",
    "De acuerdo, pues ya sabemos qué es un tensor y para que sirve: ¡para mantener las notas de vuestros exámenes bien guardadas! ;D\n",
    "\n",
    "Si queréis profundizar en los tensores o más ejemplos, aquí tenéis un recurso muy bueno para ello (en inglés): [Tensors ilustrated with cats](https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2-EEjlWDYgt"
   },
   "source": [
    "Antes os he comentado que en TensorFlow, primero se definen las operaciones a realizar y luego se ejecutan. Para ello, se usa un grafo.\n",
    "\n",
    "**¿Y qué es un grafo?**\n",
    "\n",
    "Un ejemplo sencillo de una suma de a + b.\n",
    "\n",
    "<img src=\"https://image.ibb.co/nkwp3y/tf_graph_2.png\" alt=\"tf_graph_2\" border=\"0\" height=\"200\">\n",
    "\n",
    "Y aquí tenéis un ejemplo un poco más complicado, para los valientes:\n",
    "\n",
    "![alt text](https://www.tensorflow.org/images/tensors_flowing.gif)\n",
    "\n",
    "Es un ejemplo de un grafo que representa la clasificación de una imagen de un número en su correspondiente clase. No es necesario que entendáis lo que está ocurriendo, simplemente que tensorflow funciona así: primero tu defines las operaciones que quieres que se realicen, junto a las variables necearias (creas el grafo) y luego lo ejecutas (con una sesión). Pero basta de cháchara, ¡vamos a ponernos manos a la obra!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Dl8M2wYaGmu1"
   },
   "outputs": [],
   "source": [
    "# Lo primero que debemos hacer es importar el paquete de Tensorflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt # importamos también pyplot para gráficas\n",
    "%matplotlib inline\n",
    "\n",
    "# Es muy importante que conozcamos 3 conceptos básicos de TF:\n",
    "# tf.Graph: representa un conjunto de tf.Operations\n",
    "# tf.Operation: son las operaciones indicadas por las ecuaciones que escribimos\n",
    "# tf.Tensor: los resultados de las tf.Operations\n",
    "\n",
    "# En un principio, el tf.Graph es transparente a nosotros, ya que por defecto\n",
    "# existe uno donde se van añadiendo todas las operaciones que definimos:\n",
    "# tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1528644988394,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113131840338433251202"
     },
     "user_tz": -60
    },
    "id": "7SX9jnzhF9hb",
    "outputId": "149d5831-9f83-4a45-f024-bf751c3b858f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Vamos a empezar con algo muy sencillo, una simple multiplicación en TensorFlow\n",
    "\n",
    "# Primero definimos los valores que queremos utilizar\n",
    "x = tf.constant(6)  # tf.Constant porque no va a cambiar durante la ejecución\n",
    "y = tf.constant(8)\n",
    "\n",
    "# Ahora definimos la operación a realizar: la multiplicación\n",
    "result = tf.multiply(x, y)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1176,
     "status": "ok",
     "timestamp": 1528645001309,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113131840338433251202"
     },
     "user_tz": -60
    },
    "id": "uPy3q8bZMHkp",
    "outputId": "0736db6b-b5be-4b82-f021-50947300c204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# Como podéis ver, no nos ha devuelto el resultado. Lo que ha hecho hasta ahora\n",
    "# ha sido crear el grafo. Por poner un ejemplo, es como montar un coche. Ahora \n",
    "# lo tenemos montado, pero aún no hace aquello para lo que fue diseñado, \n",
    "# desplazarse. Para ello, deberíamos encenderlo. Pues con esto es igual,\n",
    "# tenemos que encenderlo:\n",
    "\n",
    "sess = tf.Session()  # abrimos nuestro \"coche\" y lo encendemos\n",
    "output = sess.run(result)  # nos ponemos en movimiento\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "utm2VLxzWwWj"
   },
   "outputs": [],
   "source": [
    "# Para poder visualizar el grafo es necesario definir un par de funciones que \n",
    "# lo permitan. No es necesario que les prestéis atención.\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1528645012443,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113131840338433251202"
     },
     "user_tz": -60
    },
    "id": "rNwByfRnW-xu",
    "outputId": "60793914-1810-4936-98df-607dce6bb90f"
   },
   "outputs": [],
   "source": [
    "# ahora sí, visualizamos el grafo recien construido:\n",
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OFDhMkz78NoB",
    "oLJb8m8c8TSH",
    "i82s0YnP8XPH"
   ],
   "default_view": {},
   "name": "(S) 1. Introducción al Deep Learning.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
